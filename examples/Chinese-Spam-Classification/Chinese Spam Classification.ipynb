{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文垃圾邮件分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HJQ\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import gensim\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    '''\n",
    "    获取数据\n",
    "    :return: 文本数据，对应的labels\n",
    "    '''\n",
    "    with open(\"ham_5000.utf8\", encoding=\"utf8\") as ham_f, open(\"spam_5000.utf8\", encoding=\"utf8\") as spam_f:\n",
    "        ham_data = ham_f.readlines()\n",
    "        spam_data = spam_f.readlines()\n",
    " \n",
    "        ham_label = np.ones(len(ham_data)).tolist()\n",
    "        spam_label = np.zeros(len(spam_data)).tolist()\n",
    " \n",
    "        corpus = ham_data + spam_data\n",
    " \n",
    "        labels = ham_label + spam_label\n",
    " \n",
    "    return corpus, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#取空文件\n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for docs, label in zip(corpus, labels):\n",
    "        #移除字符串头尾指定的字符(默认为空格)\n",
    "        if docs.strip():\n",
    "            filtered_corpus.append(docs)\n",
    "            filtered_labels.append(label)\n",
    "    return filtered_corpus, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载停用词\n",
    "with open(\"cnstopwords.txt\", encoding=\"utf8\") as f:\n",
    "    stopword_list = f.readlines()\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = jieba.cut(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去除特殊符号\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去除英文字母\n",
    "def remove_english_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[a-zA-Z]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去除数字和空格\n",
    "def remove_numbers(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[\\d]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去除停用词\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ''.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''提取特征'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def bow_extractor(corpus, ngram_range=(1, 1)):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    transformer = TfidfTransformer(norm='l2',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_extractor(corpus, ngram_range=(1, 1)):\n",
    "    vectorizer = TfidfVectorizer(min_df=1,\n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_predict_evaluate_model(classifier,\n",
    "                                 train_features, train_labels,\n",
    "                                 test_features, test_labels):\n",
    "    # build model\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features)\n",
    "    # evaluate model prediction performance\n",
    "    get_metrics(true_labels=test_labels,\n",
    "                predicted_labels=predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('准确率:', np.round(\n",
    "        metrics.accuracy_score(true_labels,\n",
    "                               predicted_labels),\n",
    "        2))\n",
    "    print('精度:', np.round(\n",
    "        metrics.precision_score(true_labels,\n",
    "                                predicted_labels,\n",
    "                                average='weighted'),\n",
    "        2))\n",
    "    print('召回率:', np.round(\n",
    "        metrics.recall_score(true_labels,\n",
    "                             predicted_labels,\n",
    "                             average='weighted'),\n",
    "        2))\n",
    "    print('F1得分:', np.round(\n",
    "        metrics.f1_score(true_labels,\n",
    "                         predicted_labels,\n",
    "                         average='weighted'),\n",
    "        2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的数据量： 10001\n",
      "样本之一： 北京售票员可厉害，嘿嘿，有专座的，会直接拉着脖子指着鼻子让上面的人站起来让 座的，呵呵，比较赞。。。 杭州就是很少有人给让座，除非司机要求乘客那样做。 五一去杭州一个景点玩，车上有两个不到一岁的小孩，就是没有人给让座，没办法家长只能在车上把小孩的推车打开让孩子坐进去，但是孩子还是闹，只能抱着，景点离市区很远，车上很颠，最后家长坐在地上抱孩子，就是没有一个人给让座，要是在北京，一上车就有人让座了\n",
      "\n",
      "实际： 正常邮件 垃圾邮件\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "corpus, labels=get_data()  \n",
    "#获取数据集\n",
    "print(\"总的数据量：\",len(labels))\n",
    "corpus,labels=remove_empty_docs(corpus,labels)\n",
    "print(\"样本之一：\",corpus[10])\n",
    "label_name_map=[\"垃圾邮件\",\"正常邮件\"]\n",
    "print(\"实际：\",label_name_map[int(labels[10])],label_name_map[int(labels[5900])])      \n",
    "#对数据进行划分\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus, labels,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['讲的是孔子后人的故事。一个老领导回到家乡，跟儿子感情不和，跟贪财的孙子孔为本和睦。 老领导的弟弟魏宗万是赶马车的。 有个洋妞大概是考察民俗的，在他们家过年。 孔为本总想出国，被爷爷教育了。 最后，一家人基本和解。 顺便问另一类电影，北京青年电影制片厂的。中越战背景。一军人被介绍了一个对象，去相亲。女方是军队医院的护士，犹豫不决，总是在回忆战场上负伤的男友，好像还没死。最后 男方表示理解，归队了。\\n',\n",
       " '不至于吧，离开这个破公司就没有课题可以做了？ 谢谢大家的关心，她昨天晚上睡的很好。MM她自己已经想好了。见机行事吧，拿到相关的能出来做论文的材料，就马上辞职。 唉！看看吧，说不定还要各为XDJM帮出出找工作的主意呢。MM学通信的，哈尔滨工程大学的研究生，不想在哈碌碌无为的做设计，因此才出来的。先谢谢了啊。！！！ 本人语文不好，没加标点。辛苦那些看不懂的XDJM么了。\\n',\n",
       " '生一个玩玩，不好玩了就送人 第一，你要知道，你们恋爱前，你爹妈对她是毫无意义的。没道理你爹妈就要求她生孩子，她就得听话。换句话说，你岳父母要未来孩子跟妈姓，你做的到吗？夫妻是平等的。如果你没办法答应岳父母，她干吗答应你爹妈呢？ 第二，有了孩子你养不养的起？不是说想生就生，图你爹妈一个高兴，如果没有房子，没有充足的财力，生孩子只会带给你们更多的困难，生小孩容易，养小孩难啊。\\n',\n",
       " '微软中国研发啥？本地化？ 新浪科技讯 8月24日晚10点，微软中国对外宣布说，在2006财年(2005年7月-2006年6月)，公司将在中国招聘约800名新员工。 其中，一半以上的新聘人员将为研发人员，其他将是销售、市场和服务人员。同时，有近300个职位将面向新毕业的大学本科生、硕士研究生、MBA和博士生。 在2005财年，微软在中国的业务取得了骄人成绩，成为微软全球增长速度最快的子公司之一。\\n',\n",
       " '要是他老怕跟你说话耽误时间 你可得赶紧纠正他这个观点 标  题: Re: 今天晚上的事情，有点郁闷 这个...其实以前有问题的时候都是当面解决，后来他说你有什么想不通的可以到板上去 问问别人，然后你就知道是谁不对了，所以这次我就来问，我觉得挺好，避免正面冲突， 他最怕耽误他时间，这样正好也不耽误他时间，也解了我的心结 : 感觉这两人都不够坦诚 : gg郁闷了就找mm别扭 : mm别扭了就到版上哭诉 -- 淡泊以明志，宁静以至远\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\HJQ\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.432 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "norm_train_corpus=normalize_corpus(train_corpus)\n",
    "norm_test_corpus=normalize_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['尊敬', '的', '公司', '您好', '！', '打扰', '之处', '请', '见谅', '！', '我', '深圳', '公司', '愿', '在', '互惠互利', '、', '诚信为本', '代开', '3', '厘', '2', '点', '国税', '、', '地税', '等', '发票', '。', '增值税', '和', '海关', '缴款', '书', '就', '以', '2', '点', '7', '点来', '代开', '。', '手机', '：', '13510631209', '联系人', '：', '邝', '先生', '邮箱', '：', 'ao998163com', '祥细', '资料', '合作', '告知', '，', '希望', '合作', '。', '谢谢', '！', '！'], ['您好', '：', '如此', '条', '信息', '对', '您', '的', '打扰', '表示歉意', '，', '并', '请谅解', '。', '现我司', '有', '：', '商品销售', '、', '运输', '、', '建筑', '、', '广告', '等', '发票', '可代开', '，', '点数', '低', '，', '如需', '请', '电', '：', '13927401103', '刘生'], ['棒', '老公', '成人', '用品', '网', '，', '为', '您', '提供', '国内', '最新', '最', '实惠', '的', '产品', '用品', '！', '网址', '：', 'httpwww8919470com'], ['发信人', 'PMHT', '加油', '啊', '信区', 'DEE', '标题', '急寻', '明日', '出差', '去', '深圳', '的', '同学', '老师', '帮忙', '最好', '是', '中午', '或', '上午', '航班', '的', '，', '帮', '我', '捎带', '一', '重要', '证件', '前往', '深圳', '，', '对方', '在', '机场', '候着', '！', '！', '！', '由于', '是', '明天', '需要', '该', '证件', '，', '所以', '快递', '公司', '可能', '来不及', '的', '。', '交', '陌生人', '又', '不', '太', '放心', '，', '只有', '看看', '校园', '里边', '有没有', '同学', '老师', '可以', '帮忙', '的', '啦', '联系电话', '5153718813466605114', '刘'], ['这个', '说法', '我', '非常', '认同', '，', '它', '就是', '有点', '非牛', '非马', '。', '“', '养殖', '克隆人', '以', '贩卖', '器官', '”', '这样', '的', '创意', '，', '直接', '关联', '观众', '道德感', '，', '主体', '意识', '；', '你', '看到', '这个', '主题', '，', '下意识', '得', '就', '进入', '了', '道德', '批判', '状态', '，', '理性', '反思', '状态', '，', '象', '看到', '奥斯威', '信', '集中营', '一样', '；', '导演', '试图用', '明媚', '的', '色调', '讲故事', '，', '哪有', '那么', '容易', '？', '它', '挑战', '的', '是', '每个', '人', '的', '超', '我', '。', '这样', '就', '导致', '影片', '出来', '后', '基色', '非常', '模糊', '，', '反思', '派', '和', '娱乐', '派', '都', '不', '喜欢', '这', '片子', '倒霉', '就', '倒霉', '在', '想要', '的', '东西', '都', '杂', '和', '在', '一起', '了', '观众', '没法', '各取所需'], ['北京', '包月', '电话', '您好', '：', '现', '推出', '一种', '包月', '电话', '，', '每月', '只', '需', '100', '元', '，', '市话', '随便', '打', '，', '传真', '随便', '发', '，', '长途', '只', '需', '01', '元', '分钟', '，', '只限', '北京', '使用', '，', '只能', '打出', '，', '不能', '打入', '）', '详情', '：', 'httpguhuaismenet', '电话', '：', '13366138017', '联系人', '：', '金', '先生', '。', '致礼', '！', '北京', '网通', '2005', '年', '08', '月', '15', '日'], ['2', '、', '食堂', '永远都是', '一样', '的', '菜', '，', '所以', '我', '永远', '只', '吃', '那', '几个', '。', '附近', '的', '饭馆', '去', '得', '多', '了', '也', '没劲', '。', '每天', '要花', '最少', '半小时', '想', '吃', '什么', '：', '（', '如果', '有人', '有', '灶', '以及', '柴米油盐', '辣椒', '醋', '，', '且', '厨房', '卫生', '环境', '好', '，', '最好', '借', '我', '试验', '一下', '我', '是否', '能', '自己', '做', '因为', '没', '做', '过', '3', '、', '为什么', '我', '不能', '发站', '内', '信件', '？', '但', '一月', '以前', '能收', '（', '最近', '能', '不能', '收', '没', '实验', '，', '不', '知道', '）', '。', '有', '谁', '知道', '这个', '是', '为什么', '吗', '？', '谢谢', '知情人', '：', '）', '第一个', '问题', '比较', '重要', '，', '第二个', '问题', '偶尔', '出现', '困扰', '。'], ['尊敬', '的', '客户', '您好', '中国东方航空公司', '上海', '售票处', '销售', '国内外', '特价机票', '38', '折', '可', '随时', '来电', '来', '邮', '咨询', '上海市', '内', '24', '小时', '免费送', '票', '地址', '上海市', '闸北区', '天目', '东路', '206', '号', '山水', '航空', '服务', '有限公司', '电话', '021512709655127096751270969', '传真', '51270969QQ76482909MSNmajunxang8888msncomEmailmajunxiang8888126com']]\n",
      "[['小姑娘', '，', '夫妻', '之间', '没有', '对错', '之分', '，', '只有', '和睦', '与否', '当然', '，', '如果', '不想', '过', '了', '，', '另', '说', '标题', 'Re', '这样', '的', '男人', '还', '可以', '一起', '走', '下去', '吗', '？', '？', '？', '哭', '是', '懦弱', '的', '表现', '，', '是', '认错', '的', '表现', '，', '我', '不会', '，', '因为', '我', '没有', '错', '。', '。', '没事', '没事', '呵呵', '你', '不要', '跟', '她', '争', '你', '在', '他', '面前', '哭哭啼啼', '的', '觉得', '委屈', '说不定', '更', '有效'], ['我', '看', '得', '未', '剪辑版', '也', '挺', '好', '的', '啊', '没有', '什么', '不', '舒服', '那种', '感情', '我', '觉得', '可以', '理解', '最后', '把', 'mm', '按下', '通风管', '哭', '得', '挺', '感人', '的', '事实上', '剪辑版', '和', '公映', '版', '最大', '的', '区别', '，', '不', '在于', '带', '小女孩', '杀人', '不', '杀人', '，', '而', '在于', '影片', '对于', 'leon', '和', '小女孩', '关系', '的', '交代', '。', '未', '剪辑版', '里', '，', '有', '小女孩', '对', 'leon', '说', 'Iloveu', '，', '而且', '还', '希望', 'leon', '能', '拥有', '她', '的', '情节', '，', '比较', '直白', '，', '平淡', '。', '而', '公映', '版', '，', '二人', '是', '介于', '父女', '和', '情', '人间', '，', '让', '人', '无法', '区分', '，', '而', '又', '特别', '感人', '的', '情感', '。'], ['3', '、', '免费', '获取', 'VIP', '贵宾卡', '，', '参加', '在线', '学习', '；', '获得', '方法', '：', '致电', '给', '我们', '：', '02061138856', '左', '小姐', '，', '或者', '告知', '您', '的', '联系方式', '，', '您', '将', '能够', '了解', '到', '不', '依赖', '网速', '快速', '拥有', '300', '多门', '经典', '课程', '的', '最新', '学习', '方法', '，', '详细', '请', '访问', '：', 'httpwwwgzemscom'], ['贵', '公司', '负责人', '：', '你好', '！', '本', '公司', '祥泰', '实业', '有限公司', '）', '具有', '进出口', '及', '国内贸易', '的', '企业', '承', '多家', '公司', '委托', '有', '广告', '建筑工程', '其它', '服务', '商品销售', '等', '的', '发票', '向', '外代', '开', '点数', '优惠', '本', '公司', '原则', '是', '满意', '后', '付款', '有意者', '请来', '电', '洽谈', '电话', '：', '013631690076', '邮箱', '：', 'shitailong8163com', '联系人', '：', '郭生', '如', '给', '贵', '公司', '带来', '不便', '请谅解'], ['深圳市', '顺发', '电子', '美国', '有限公司', '网址', 'httpwwwmgqtcom', '公司', '产品', '有', '购买', '窃听器', '．', '手机', '窃听器', '．', '美国', '监听', '王', 'GSM8', '移动电话', '拦截', '系统', '．', '电子', '变牌', '电表', '控制器', 'zx003', '．', '电话', '变声', '器', '．', '游戏机', '反', '遥控', '隐形', '汽车', '牌照', 'as240', '．', '等', '实惠', '产品', '：', '24', '小时', '订购电话', '：', '07556119992624', '小时', '联系人', '：', '陈先生'], ['欧洲', '也', '没有', '吗', '？', '标题', 'Re', '为什么', 'MIT', '和', '水木', '对待', '王垠', '的', '态度', '不同', '？', '如果', '把', '土壤', '等同于', '没有', '发', '文章', '的', '压力', '，', '那么', '国外', '也', '没有', '这种', '土壤', '呵呵', '目前', '的', '清华', '本身', '有', '培养', '你', '说', '的', '那种', '大师', '的', '土壤', '么', '？', '如果', '没有', '，', '那', '就是', '区别', '，', '本质', '的', '区别', '其实', '你', '用', '脚', '投票', '这一', '事实', '，', '本身', '已经', '反驳', '了', '你', '的', '论点'], ['可能', '因为', 'gg', '要', '为', '那些', '担忧', '，', '所以', '他', '肯定', '也', '不能', '体会', '我', '的', '痛苦', '，', '以为', '我', '就是', '闲', '的', '。', '既然', '这样', '，', '我痛', '又', '没人', '知道', '，', '还', '不如', '化', '悲痛', '为', '力量', '，', '说不定', '还', '有点', '用', '呢', '。', '标题', 'Re', '我', '又', '逼', 'gg', '了', '没错', '，', '觉得', '还是', '有点', '太闲', '了', '，', '如果', '每天', '要', '为', '衣食住行', '担忧', '，', '就', '不会', '这样', '了', '找点', '事情', '做', '比较', '好', '，', '干吗', '掉', '在', '一颗', '树上', '啊', '好', '男人', '多', '了', '去', '了', '。', '删', '了', '我', '就', '不', '知道', '了', '吗', '？', '今日', '是', '你', '大喜', '的', '日子', '，', '我', '怎能不', '来', '？'], ['其实', '就是', '跟', 'office', '一样', '的', '一个', '写作', '工具', '而已', '，', '有', '谁', '因为', 'word', '用', '的', '熟', '而', '觉得', '自己', '很', '牛', '的', '标题', 'Re', '现在', '牛', '人', '标准', '降', '的', '很', '低', '了', '啊', 'Re', '为什么', '大家', '盲目崇拜', '王垠', '？', '？', '？', 'hehe', '我们', '组里', '的', '人', '都', '会', '使', 'TEX', '，', '没', '觉得', '自己', '很', '牛', '啊', '。', '没', '看出', '王垠', '同学', '牛', '在', '哪儿', '博士', '读', '到', '四年', '有', '两个', 'conference', '的', 'paper', '一个', '是', '二三流', '会议', '的', 'BPW', '自己', '老板', '是', 'chair', '而且', '还是', '国内', '开', '的', '国际', '会议']]\n"
     ]
    }
   ],
   "source": [
    "#词袋模型特征\n",
    "bow_vectorizer,bow_train_features=bow_extractor(norm_train_corpus)\n",
    "bow_test_features=bow_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "#tfidfT特征\n",
    "tfidf_vectorizer,tfidf_train_features=tfidf_extractor(norm_train_corpus)\n",
    "tfidf_test_features=tfidf_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "#tokenize documents\n",
    "tokenized_train=[jieba.lcut(text)  for text in norm_train_corpus]\n",
    "print(tokenized_train[2:10])\n",
    "tokenized_test=[jieba.lcut(text)  for text in norm_test_corpus]\n",
    "print(tokenized_test[2:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.模型结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于词袋模型特征的贝叶斯分类器\n",
      "准确率: 0.96\n",
      "精度: 0.96\n",
      "召回率: 0.96\n",
      "F1得分: 0.96\n",
      "基于词袋模型特征的逻辑回归\n",
      "准确率: 0.95\n",
      "精度: 0.95\n",
      "召回率: 0.95\n",
      "F1得分: 0.95\n",
      "基于词袋模型的支持向量机\n",
      "准确率: 0.96\n",
      "精度: 0.96\n",
      "召回率: 0.96\n",
      "F1得分: 0.96\n",
      "基于tfidf的贝叶斯模型\n",
      "准确率: 0.95\n",
      "精度: 0.96\n",
      "召回率: 0.95\n",
      "F1得分: 0.95\n",
      "基于tfidf的逻辑回归模型\n",
      "准确率: 0.93\n",
      "精度: 0.94\n",
      "召回率: 0.93\n",
      "F1得分: 0.93\n",
      "基于tfidf的支持向量机模型\n",
      "准确率: 0.96\n",
      "精度: 0.96\n",
      "召回率: 0.96\n",
      "F1得分: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge')\n",
    "lr = LogisticRegression()\n",
    "# 基于词袋模型的多项朴素贝叶斯\n",
    "print(\"基于词袋模型特征的贝叶斯分类器\")\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                                   train_features=bow_train_features,\n",
    "                                                   train_labels=train_labels,\n",
    "                                                   test_features=bow_test_features,\n",
    "                                                   test_labels=test_labels)\n",
    "\n",
    "# 基于词袋模型特征的逻辑回归\n",
    "print(\"基于词袋模型特征的逻辑回归\")\n",
    "lr_bow_predictions = train_predict_evaluate_model(classifier=lr,\n",
    "                                                  train_features=bow_train_features,\n",
    "                                                  train_labels=train_labels,\n",
    "                                                  test_features=bow_test_features,\n",
    "                                                  test_labels=test_labels)\n",
    "\n",
    "# 基于词袋模型的支持向量机方法\n",
    "print(\"基于词袋模型的支持向量机\")\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                   train_features=bow_train_features,\n",
    "                                                   train_labels=train_labels,\n",
    "                                                   test_features=bow_test_features,\n",
    "                                                   test_labels=test_labels)\n",
    "\n",
    "\n",
    "# 基于tfidf的多项式朴素贝叶斯模型\n",
    "print(\"基于tfidf的贝叶斯模型\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                                     train_features=tfidf_train_features,\n",
    "                                                     train_labels=train_labels,\n",
    "                                                     test_features=tfidf_test_features,\n",
    "                                                     test_labels=test_labels)\n",
    "# 基于tfidf的逻辑回归模型\n",
    "print(\"基于tfidf的逻辑回归模型\")\n",
    "lr_tfidf_predictions=train_predict_evaluate_model(classifier=lr,\n",
    "                                                     train_features=tfidf_train_features,\n",
    "                                                     train_labels=train_labels,\n",
    "                                                     test_features=tfidf_test_features,\n",
    "                                                     test_labels=test_labels)\n",
    "\n",
    "\n",
    "# 基于tfidf的支持向量机模型\n",
    "print(\"基于tfidf的支持向量机模型\")\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                     train_features=tfidf_train_features,\n",
    "                                                     train_labels=train_labels,\n",
    "                                                     test_features=tfidf_test_features,\n",
    "                                                     test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "贵公司负责人：你好！ 本公司(祥泰实业有限公司）具有进出口及国内贸易的企业, 承多家公司委托:有广告/建筑工程/其它服务/商品销售/...等的 发票向外代开.点数优惠,本公司原则是满意后付款.有意者请来 电洽谈. 电  话：013631690076 邮  箱：shitailong8@163.com 联系人：郭 生 如给贵公司带来不便请谅解! \n",
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "深圳市顺发电子(美国)有限公司 网址:http://www.mgqt.com 公司产品有:购买窃听器．手机窃听器．美国监听王. GSM_8移动电话拦截系统．电子变牌. 电表控制器zx003．电话变声器．游戏机反遥控. 隐形汽车牌照as240．等实惠产品： (24)小时订购电话：0755-61199926 (24)小时联系人：陈先生 \n",
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "你好： 真正自己国人的，免费注册，有偿回报的系统！ 通宝：最佳网络伴侣 闲余间还能为你增加收入！！ 绿色软件，注册简单，而且免费： Http://rich138.008.net 赠送《超高能潜意识CD》 只需购买以下课程200元，即赠送一套！！ Http://www.cg-biz.com/list.txt \n",
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "您好： 我是广州市实达贸易有限公司,现有剩余(广告.运输.服务.商品.) 等各种普通发票可以代开,只收2% 的税点 联 人: 张高伟 手 机:  13828415779              (先验票后付款) 电 话:  020-33622299 回 邮:  zz8cle@avl.com.cn \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "极度yy啊~~ 前言：已经很久没有熬夜了，深夜总是我们敞开灵魂的时刻。温温柔柔的写了一个东东给网友mm寄过去，不知道她会不会误会，以为我喜欢她。希望大家给点意见。 琳，你好： 我的手机已经发光包月，现在只能发信问候。其实写信也是和有情致的事情。只是现代的科技把我们推向了平庸和无奇。我不喜欢没有变化的世界，我不喜欢没有期待的日立，我不喜欢没有云雨的天际，我不喜欢没有信札的联系。所以，我给你写信。 \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "现招聘兼职，全职老师，英语，物理，化学等各类老师！！ 工作地点在望京新城，主要教外国人中文和辅导小孩！！这里是朋友你好 （北京）学院！！ 有意请与我联系・（长期有效） 联系人：崔小姐 电话：84713126 \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "发信人: xiaoyanliu (挪威的宁静), 信区: THUInfo 标  题: 再次说明：关于牛志升老师讲座的ppt 发信人: xiaoyanliu (挪威的宁静), 信区: DEE 标  题: 再次说明：关于牛志升老师讲座的ppt 如果大家想索取ppt，直接发到我的邮箱就可以了，xy-liu04@mails.tsinghua.edu.cn 就不要发到牛老师那里去了，谢谢合作！ 我们将在组织下一次讲座考虑到这种情况，谢谢 \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "这个的标题好像是，我相信他是真的爱我的。。。 不过我还是觉得这明明就是男的企图诱奸，女的顺水推舟 翻了半天没啥有潜力的启示.... 我和他是在两个城市，因为他在我住的这个城市有业务，所以经常过来这边办事，我和他的业务一点也扯不上关系，只是他认识我的老板，偶而到我们店里来几次，当时只是属于见面说一声“你好”的朋友，不，应该说那时候连朋友都算不上，后来在一个偶然后的机会，他从我们老板 \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "label_name_map=[\"垃圾邮件\",\"正常邮件\"]\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "      if label == 0 and predicted_label == 0:\n",
    "        print('邮件类型:', label_name_map[int(label)])\n",
    "        print('预测的邮件类型:', label_name_map[int(predicted_label)])\n",
    "        print('文本:-')\n",
    "        print(re.sub('\\n', ' ', document))\n",
    "\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "     if label == 1 and predicted_label == 0:\n",
    "        print('邮件类型:', label_name_map[int(label)])\n",
    "        print('预测的邮件类型:', label_name_map[int(predicted_label)])\n",
    "        print('文本:-')\n",
    "        print(re.sub('\\n', ' ', document))\n",
    "\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.探索性方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#词袋模型\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# CountVectorizer类会把文本全部转换为小写，然后将文本词块化。主要是分词，分标点\n",
    "data_train_cnt = vectorizer.fit_transform(train_corpus)\n",
    "data_test_cnt = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#变成TF-IDF矩阵\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "data_train_tfidf = transformer.fit_transform(data_train_cnt)\n",
    "data_test_tfidf = transformer.transform(data_test_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF score:  0.8760413195601466\n"
     ]
    }
   ],
   "source": [
    "# 随机森林\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0, n_estimators=100, max_depth=None, verbose=0, n_jobs=-1)\n",
    "rf.fit(data_train_tfidf, train_labels)\n",
    "score = rf.score(data_test_tfidf, test_labels)\n",
    "print(\"RF score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM score:  0.9726179655785464\n"
     ]
    }
   ],
   "source": [
    "# LightGBM的方法\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier()\n",
    "# lgb_clf.fit(data_train_tfidf, label_train)\n",
    "# 使用网格搜索得到适当参数\n",
    "param_test = {\n",
    "    'max_depth': range(2, 3)\n",
    "}\n",
    "gsearch = GridSearchCV(estimator=lgb_clf, param_grid=param_test, scoring='roc_auc', cv=5)\n",
    "gsearch.fit(data_train_tfidf,train_labels)\n",
    "# print(gsearch.best_params_)\n",
    "score = gsearch.score(data_test_tfidf, test_labels)\n",
    "print(\"LGBM score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HJQ\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:10:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost score:  0.9596801066311229\n"
     ]
    }
   ],
   "source": [
    "# XGBoost方法\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=100, n_jobs=-1, max_depth=15, min_child_weight=3, colsample_bytree=0.4)\n",
    "xgb_clf.fit(data_train_tfidf,train_labels)\n",
    "score = xgb_clf.score(data_test_tfidf, test_labels)\n",
    "print(\"XGBoost score: \", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
